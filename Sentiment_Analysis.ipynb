{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have created a model using Naive Bayes Classifier to classify a text as positive, negative or neutral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import all the libraries you will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training our model, we have used nltk's twitter dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[:1500]\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')[:1500]\n",
    "neutral_tweet_tokens = twitter_samples.tokenized('tweets.20150430-223406.json')[:3000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "1500\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_tweet_tokens))\n",
    "print(len(negative_tweet_tokens))\n",
    "print(len(neutral_tweet_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below - clean() and lemmatize_sentence() help in cleaning out the data to remove stopwords, urls, single-character tokens, punctuations. \n",
    "\n",
    "We do this using regex and WordNetLemmatizer.\n",
    "\n",
    "Since lemmatization returns an actual word of the language, it takes a lot of time to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokens):\n",
    "    data_clean = []\n",
    "\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;\\!\\'\\/?,#\\\"\\[\\]]\")\n",
    "\n",
    "    for data in tokens:\n",
    "        token = REPLACE_NO_SPACE.sub(\"\", data.lower())\n",
    "        token = re.sub('http[s:]//(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token= re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        token=re.sub(\"([0-9_]+)\",\"\", token)\n",
    "        if token not in stopwords.words() and len(token)>1 and token not in string.punctuation:\n",
    "            data_clean.append(token)\n",
    "\n",
    "    lem = lemmatize_sentence(data_clean)\n",
    "\n",
    "    return lem\n",
    "\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        #NN - noun, VB - verb\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation:\n",
    "            lemmatized_sentence.append(token.lower())\n",
    "\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean our positive, negative and neutral datasets with the help of the functions created above.\n",
    "\n",
    "This takes some time depending on your dataset sample. Please wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "neutral_cleaned_tokens_list = []\n",
    "\n",
    "for i in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(clean(i))\n",
    "for i in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(clean(i))\n",
    "for i in neutral_tweet_tokens:\n",
    "    neutral_cleaned_tokens_list.append(clean(i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anyway', ':-)']\n"
     ]
    }
   ],
   "source": [
    "print(positive_cleaned_tokens_list[200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned the data, we create our data model using the get_data_model() function we created above.\n",
    "The Naive Bayes Classifier requires not just a list of words, but a Python dictionary with words as keys and True as values.\n",
    "\n",
    "We take a threshold level of 0.8 to separate our data into training sample and testing sample, ie. 80% training data and 20% testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens_for_model = get_data_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_data_model(negative_cleaned_tokens_list)\n",
    "neutral_tokens_for_model = get_data_model(neutral_cleaned_tokens_list)\n",
    "\n",
    "positive_dataset = [(data, \"Positive\") for data in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(data, \"Negative\") for data in negative_tokens_for_model]\n",
    "\n",
    "neutral_dataset = [(data, \"Neutral\") for data in neutral_tokens_for_model]\n",
    "\n",
    "threshold = 0.8\n",
    "pos_len = int(threshold*len(positive_dataset))\n",
    "neg_len = int(threshold*len(negative_dataset))\n",
    "neu_len = int(threshold*len(neutral_dataset))\n",
    "\n",
    "train_data = positive_dataset[:pos_len] + negative_dataset[:neg_len] + neutral_dataset[:neu_len]\n",
    "test_data = positive_dataset[pos_len:] + negative_dataset[neg_len:] + neutral_dataset[neu_len:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have used Naive Bayes Classifier to train our model. \n",
    "And we have stored this trained model in a pickle file in our local storage, to give a faster response in our API when we test the sentiment of a user provided string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 99.41666666666666\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify, NaiveBayesClassifier\n",
    "import pickle as pickle\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data)*100)\n",
    "\n",
    "pickle.dump(classifier, open('models/final_prediction.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = 'models/final_prediction.pickle'\n",
    "model = pickle.load(open(modelfile, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(text):\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = (tokenizer.tokenize(text))\n",
    "    prediction = model.classify(dict([token, True] for token in tokens))\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Neutral\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"This was an amazing movie!\"\n",
    "text_2 = \"It was average\"\n",
    "text_3 = \"Life is terrible.\"\n",
    "\n",
    "print(find(text_1))\n",
    "print(find(text_2))\n",
    "print(find(text_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
